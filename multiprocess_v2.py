from datetime import timedelta
import os
import json
import time
from typing import Dict
import random
from tqdm import tqdm
import multiprocessing
import openai
import logging


class LogFormatter():

    def __init__(self):
        self.start_time = time.time()

    def format(self, record):
        elapsed_seconds = round(record.created - self.start_time)

        prefix = "%s - %s - %s" % (
            record.levelname,
            time.strftime('%x %X'),
            timedelta(seconds=elapsed_seconds)
        )
        message = record.getMessage()
        message = message.replace('\n', '\n' + ' ' * (len(prefix) + 3))
        return "%s - %s" % (prefix, message) if message else ''

def create_logger(filepath=None, rank=0, name=None):
    """
    Create a logger.
    Use a different log file for each process.
    filepath ä¸ºNoneçš„æ—¶å€™å³ä¸è¾“å‡ºåˆ°æ–‡æœ¬é‡Œé¢å»ï¼Œ
    rankä¸º0çš„æ—¶å€™å³å•çº¿ç¨‹
    """
    # create log formatter
    log_formatter = LogFormatter()

    # create file handler and set level to debug
    if filepath is not None:
        if rank > 0:
            filepath = '%s-%i' % (filepath, rank)
        file_handler = logging.FileHandler(filepath, "a")
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(log_formatter)

    # create console handler and set level to info
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(log_formatter)

    # create logger and set level to debug
    if name != None:
        logger = logging.getLogger(name)
    else:
        logger = logging.getLogger(name)
    logger.handlers = []
    logger.setLevel(logging.DEBUG)
    logger.propagate = False
    if filepath is not None:
        logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    # reset logger elapsed time
    def reset_time():
        log_formatter.start_time = time.time()
    logger.reset_time = reset_time

    return logger

logger = create_logger("./run.log")


openai.api_key = ""

# proxy = {
# 'http': 'http://localhost:4780',
# 'https': 'http://localhost:4780'
# }

# openai.proxy = proxy

def get_messages(data: Dict):

    message = f"this is your messages : {str(data)}"
    logger.info(message)                                           # æ‰“å°è°ƒç”¨prompt

    messages = {'messages': [{"role": "user", "content": message}, {"role": "system", "content": ""}]}
    return messages

def get_response(messages, temperature=0):
  response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-0613", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.
        messages=messages,
        temperature=temperature
    )
  return response['choices'][0]['message']['content']

def while_try_response(get_response_func, input):
    """åœ¨è°ƒç”¨å‡½æ•°å¤–é¢åŒ…ä¸€ä¸ªå¾ªç¯ï¼ŒæŠ¥é”™äº†å°±æ‰“å°å‡ºæ¥ï¼Œç„¶åç»§ç»­è°ƒç”¨ï¼Œç›´åˆ°æˆåŠŸ"""
    # inputå†™æˆä¸€ä¸ªå­—å…¸ï¼Œè¿™æ ·åœ¨é‡Œé¢ç›´æ¥è§£æå°±å¯ä»¥
    while True:
        try:
            result = get_response_func(**input)
            break
        except Exception as e:
            logger.critical(e)
            time.sleep(10 + random.random() * 15)                   # å‡ºé”™äº†å°±ç­‰ä¸€ä¼šå†ç»§ç»­è°ƒç”¨
    return result

def thread_fun(file_name, file_dir,  output_file, result_name):
    logger.info(f"ğŸ–ï¸ è°ƒå–æ•°æ® {file_name} ......")
    with open(os.path.join(file_dir, file_name)) as f:
        data = json.loads(f.read())                                 # è¯»å‡ºæ•°æ®
        
    messages = get_messages(data)                                   # è·å–æ¶ˆæ¯

    logger.info(messages)                                           # æ‰“å°è°ƒç”¨prompt
    result = while_try_response(get_response, messages)
    
    data[result_name] = result

    with open(f'{output_file}/{file_name}', 'w') as f:              # å­˜å…¥ç»“æœ
        json.dump(data, f, indent=4)
    logger.info(f"ğŸ”¥ {file_name} was done")


if __name__ == '__main__':
    
    openai.api_key = ""
    
    num_thread = 10
    file_dir = f'./data'                                            # è°ƒç”¨çš„æ•°æ®ä½ç½®    
    output_file = file_dir + '_result'                              # ç»“æœå†™å…¥ä½ç½®
    result_name = "res"                                             # ç»“æœå­—æ®µåå­—
    
    os.makedirs(output_file, exist_ok=True)

    logger.info('start')
    pool = multiprocessing.Pool(processes=num_thread)
    process = []

    num_slice = -1                                                  # é€‰æ‹©è°ƒå‡ æ¡ï¼Œ ç”¨äºåˆæ­¥æµ‹è¯•
    call_file_list = list(os.listdir(file_dir))
    # random.shuffle(call_file_list)                                # æ˜¯å¦éšæœºè°ƒ
    call_file_list = call_file_list[:num_slice] if num_slice > -1  else call_file_list

    logger.info(f"â—ï¸â—ï¸â—ï¸ è°ƒç”¨æ•°æ®ä¸ªæ•° {len(call_file_list)}")
    for file_name in call_file_list:
        if file_name not in os.listdir(output_file):                # åˆ¤æ–­æ˜¯å¦å·²ç»è°ƒç”¨è¿‡çš„é€»è¾‘
            logger.info(file_name)
            p = pool.apply_async(thread_fun, (file_name, file_dir, output_file, result_name))
            process.append(p)
            logger.info(f"â—ï¸ è¿˜å‰©ä¸‹ {len(call_file_list) - len(os.listdir(output_file))} æ¡æ•°æ®")
            # thread_fun(file_name, length, file_dir, output_file)
        # break
    pool.close()
    pool.join()
    
    for p in process:
        p.get()
        
    logger.info('done!!')    
        
    
        
    
        